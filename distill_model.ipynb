{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "895f7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from os.path import sep\n",
    "import io\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21314cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_pth = f\"models{sep}deepseek-r1{sep}transformers{sep}r1{sep}1\"\n",
    "MAX_NUM_SEQS = 32\n",
    "MAX_MODEL_LEN = 8192 * 3 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2fc435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_pth,\n",
    "#    dtype=\"half\",                 # The data type for the model weights and activations\n",
    "    max_num_seqs=MAX_NUM_SEQS,    # Maximum number of sequences per iteration. Default is 256\n",
    "    max_model_len=MAX_MODEL_LEN,  # Model context length\n",
    "    trust_remote_code=True,       # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n",
    "    tensor_parallel_size=4,       # The number of GPUs to use for distributed execution with tensor parallelism\n",
    "    gpu_memory_utilization=0.95,  # The ratio (between 0 and 1) of GPU memory to reserve for the model\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
